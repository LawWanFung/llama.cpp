services:
  llama-cpp-builder:
    build:
      context: .
      dockerfile: Dockerfile
      # Specify platform if building on a non-ARM64 host
      # platform: linux/arm64
    ports:
      - 8964:8080
    environment:
      - HUGGING_FACE_HUB_TOKEN= ${HF_TOKEN}
      - HF_TOKEN= ${HF_TOKEN}
      - HF_HOME=/models
      - XDG_CACHE_HOME=/models
    volumes:
      # - ./llama.cpp_output:/app/llama.cpp/build/bin # Mount to persist compiled binaries
      # - llama.cpp_data:/app/llama.cpp/build/bin
      - type: bind
        source: llama.cpp_data
        target: /app/llama.cpp/build/bin
    networks:
      - dokploy-network
# volumes:
#   llama.cpp_data:
#     driver: local
#     driver_opts:
#       type: none
#       device: /mnt/data/dokploy
#       o: bind
      
networks:
  dokploy-network:
    external: true # Indicates that this network is managed externally by Dokploy
